{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa54605",
   "metadata": {},
   "source": [
    "We are going to predict the stock price of Google. There is indeed a Brownian Motion that states the future variations of the stock price are independent of the past. So, we will try to predict the upward and downward trends that exist in Google stock price. And to do so, we will implement the LSTM model.\n",
    "\n",
    "We will make an LSTM that will try to capture the downward and the upward trend of the Google stock price because LSTM is the only powerful model that can do this as it performs way better than the traditional models. Apart from this, we are not going to perform a simple LSTM model. It's going to be super robust with some high-dimensionality, several layers as well as it is going to be a stacked LSTM, and then we will add some dropout regularization to avoid overfitting. Also, we will use the most powerful optimizer that we have in the Keras library.\n",
    "\n",
    "In order to approach this problem, we will train our LSTM model on five years of the Google stock price, which is from the beginning of 2012 to the end of 2016 and then based on this training as well as on the identified correlations or captured by the LSTM of the Google stock price, we will try to predict the first month of 2017. We will try to predict January 2017, and again we are not going to predict exactly the stock price, but we are trying to predict the upward and downward trend of the Google stock price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff3da6",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dadcc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b521d7d",
   "metadata": {},
   "source": [
    "Next, we will import the training set and not the whole set as opposed to part1 and part2 because we want to highlight that we are going to train our RNN on only the training set.\n",
    "\n",
    "The RNN will have no idea of what is going on in the test set. It will have no acquaintance with the test set during its training. It's like the test set doesn't exist for the RNN. But once the training is done, we will then introduce the test set to the RNN, so that it can make some predictions of the future stock price in January 2017. This is why we are only importing the training set now, and after the training is done, we will import the test set.\n",
    "\n",
    "So, to import the training set, we will first need to import the data as DataFrames, which we will import with pandas using the read_csv function. But then remember we have to not only select the right column that we need, which is the open Google stock price, but also, we need to make it a NumPy array because only the NumPy array can be the inputs of neural network in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce2e4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the training set  \n",
    "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')  \n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771aeff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[325.25],\n",
       "       [331.27],\n",
       "       [329.83],\n",
       "       ...,\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cac230",
   "metadata": {},
   "source": [
    "From the above, we can see that dataset_train is the DataFrame and training_set is the NumPy array of 1258 lines corresponding to 1258 stock prices in between 2012 and 2016, and one column, which is the open Google stock price.\n",
    "\n",
    "Now we can precisely check from the above image that the open Google stock price in training set with the same number of lines, i.e., the same number of stock prices. So, we have a NumPy array of one column but not the vector.\n",
    "\n",
    "After this, we will apply the feature scaling to our data to optimize the training process, and feature scaling can be done in two different ways, i.e., standardization and normalization. In standardization, we subtract the observation by the mean of all observations in one same column and then divide it by the standard deviation. However, in normalization, we subtract the observation by the minimum of all observations, i.e., the minimum stock prices, and then we divide it by the maximum of all the stock prices minus the minimum of all the stock prices.\n",
    "\n",
    "So, this time is more relevant to use normalization because whenever we build an RNN and especially if there is a sigmoid function as the activation function in the output layer of the recurrent neural network, it is recommended to apply normalization. Therefore, we will apply normalization, and to do this, we will import the min-max k-load class from the preprocessing module of the scikit learn library, and then from this preprocessing module, we will import the MinMaxScaler class.\n",
    "\n",
    "Now from this class, we will create an object of the same class, which we will call as sc for scale. And sc will be the object of MinMaxScaler class inside of which we will pass the default argument, i.e., feature_range. Here we have made feature_range equals to (0, 1) because if we look at the case of normalization, we will see that all the new scaled stock processes will be between 0 and 1, which is exactly what we want.\n",
    "\n",
    "Next, we will apply the sc object on our data to effectively apply the normalization. For this we will introduce a new variable which will be the scaled training set, so we will name it as training_set_scaled, and in order to get the normalized training set, we will simply take the sc object followed by applying fit_transform method, which is the method of MinMaxScaler class so as to fit the sc object to the training_set that we will input as an argument and then scale it. Basically, fit means that it is just going to get the min of the data, i.e., the minimum stock price and the maximum stock price to be able to the normalization formula. And then, with the transform method, it will compute for each of the stock prices of the training set, the scaled stock prices according to the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dab6393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368],\n",
       "       [0.09701243],\n",
       "       [0.09433366],\n",
       "       ...,\n",
       "       [0.95725128],\n",
       "       [0.93796041],\n",
       "       [0.93688146]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling  \n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "sc = MinMaxScaler(feature_range = (0, 1))  \n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d68e2",
   "metadata": {},
   "source": [
    "After executing the above lines of code, we will obtain our training_set_scaled, as shown in the above image. And if we have a look at it, we can see that indeed all the stock prices are now normalized between 0 and 1.\n",
    "\n",
    "In the next step, we will create a specific data structure, which is the most important step of data preprocessing for Recurrent Neural Networks. Basically, we are going to create a data structure specifying what the RNN will need to remember when predicting the next stock price, which is actually called the number of timesteps and it is very important to have a right number of timesteps because the wrong number of timesteps could lead to overfitting or baseless predictions.\n",
    "\n",
    "So, we will be creating 60 timesteps and 1 output, such that 60 timesteps mean that at each time T, the RNN is going to look back at 60 stock prices before time T, i.e., the stock prices between 60 days before time T and time T, and based on the trends, it is capturing during these 60 previous timesteps, it will try to predict the next output. So, 60 timesteps of the past information from which our RNN is going to learn and understand some correlations or some trends, and based on its understanding, it's going to try to predict the next output, i.e., the stock price at time t+1. Also, 60 timesteps refer to 60 previous financial days, and since there are 20 financial days in one month, so 60 timesteps correspond to three months, which means that each day we are going to look at the three previous months to try to predict the stock price the next day.\n",
    "\n",
    "So, the first thing that we need to create two separate entities; the first entity that we will create is X_train, which will be the inputs of the neural network, and the second will be y_train that will contain the output. Basically, for each observation, or we can say for each financial day, X_train will contain 60 previous stock prices before that financial day, and y_train contain the stock price of the next financial day. We will start initializing these two separate entities, i.e., X_train and y_train, as an empty list.\n",
    "\n",
    "The next step is for a loop because we will populate these entities with 60 previous stock prices in X_train and the next stock price in the y_train. So, we will start the loop with 60 because then for each i which is the index of the stock price observation, we will get the range from i-60 to i, which exactly contains the 60 previous stock prices before the stock price at time t. Therefore, we will start the range at 60 because then the upper bound is much easier to find, which is off course, the last index of our observation, i.e., 1258. Inside the for loop, we will start with X_train, which is presently an empty list, so we will append some elements into the X_train by using the append function. We will append the 60 previous stock prices before the stock price at index i, i.e., the stock price at the ith financial day. So, in order to get them, we will get our training_set_scaled, and in this, we will take 60 previous stock prices before the ith financial day, which is the range of the indexes from i-60 to i. Since we already selected correct lines for X_train, but we still need to specify the column and as we have one column in the scaled training set, i.e., the column of index 0, which is exactly what we need to add here.\n",
    "\n",
    "Now, in the same way, we will do for y_train, which will be much easier because we simply need to input the stock price at time t+1, and therefore we just need to do the same here. The stock price at t+1 is, of course, going to be taken from training_set_scaled and inside it we will take same indexes for columns, i.e., 0, but for the observation line, we will take the ith index because if we consider the same example when we have i equal to 60, then X_train will contain all the stock prices from 0 to 59 as the upper bound was excluded, but what we want to predict is actually based on the 60 previous stock prices is the stock price at time t+1, which is 60 and that is the reason we input i here instead of i+1.\n",
    "\n",
    "So, now we have the 60 previous stock prices in X_train and the stock price at time t+1 in y_train. Since X_trian and y_train are lists, so we again need to make them NumPy arrays for them to be accepted by our future Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02dd146d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368, 0.09701243, 0.09433366, ..., 0.07846566, 0.08034452,\n",
       "        0.08497656],\n",
       "       [0.09701243, 0.09433366, 0.09156187, ..., 0.08034452, 0.08497656,\n",
       "        0.08627874],\n",
       "       [0.09433366, 0.09156187, 0.07984225, ..., 0.08497656, 0.08627874,\n",
       "        0.08471612],\n",
       "       ...,\n",
       "       [0.92106928, 0.92438053, 0.93048218, ..., 0.95475854, 0.95204256,\n",
       "        0.95163331],\n",
       "       [0.92438053, 0.93048218, 0.9299055 , ..., 0.95204256, 0.95163331,\n",
       "        0.95725128],\n",
       "       [0.93048218, 0.9299055 , 0.93113327, ..., 0.95163331, 0.95725128,\n",
       "        0.93796041]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output  \n",
    "X_train = []  \n",
    "y_train = []  \n",
    "for i in range(60, 1258):  \n",
    "    X_train.append(training_set_scaled[i-60:i, 0])  \n",
    "    y_train.append(training_set_scaled[i, 0])  \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9d0c6",
   "metadata": {},
   "source": [
    "As we can see from the above image, X_train is a special data structure. Here the first line of observation corresponds to time t equals 60, which means it corresponds to the stock price at the 60th financial day of our training dataset. And all those values are the previous 60 stock prices before that stock price at the 60th financial day, which means that there are 59 values here, such that if we have a look at the first line, i.e., observation of the 1st index, corresponds to the stock price at the 61st financial day of the training set. All these stock prices are the preview stock prices before that 61st stock price of our training dataset.\n",
    "\n",
    "Now if we have a look at y_train, we can see it very simple to visualize as it contains the stock price at time t+1 and if compare both the X_train and y_train, we will see that X_train contains all the 60 previous stock prices t = 60, and based on the stock prices of each individual line, we will train our Recurrent Neural Network to predict the stock price at time t+1.\n",
    "\n",
    "After this, we will perform our last step of data pre-processing, which is reshaping the data, or in simple words, we can say we will add some more dimensionality to the previous data structure. And the dimensionality that we are going to add is the 'unit', i.e., the number of predictors that we can use to predict the Google stock prices at time t+1.\n",
    "\n",
    "So, in the scope of this financial engineering problem where we are trying to predict the trend of the Google stock price, these predictors are indicators. Presently we are having one indicator, which is the Google Stock Prices and so we are taking 60 previous Google stock prices to predict the next one. But with the help of a new dimension that we are going to add to our data structure, we will be able to add some more indicators that will help in predicting even better the upward and downward trends of the Google Stock Price.\n",
    "\n",
    "We will use the reshape function to add a dimension in the NumPy array. We just need to do this for X_train because it actually contains the inputs of the neural network. So, we create a new dimensionality of the new data structure because simply that is exactly what is expected by the future recurrent neural network that we are going to build in our 2nd part.\n",
    "\n",
    "So, we will start by updating the X_train by using the reshape function, which is taken from the NumPy library because we are reshaping a NumPy array. Inside the reshape function, we will input the following arguments:\n",
    "\n",
    "The first argument is the NumPy array, i.e., X_train that we want to reshape as we want to add the new dimension corresponding to the predictor, which is the indicator in our case.\n",
    "And in the second argument of the reshape function, we need to specify this new structure, i.e., the new shape we want for our X_train to have. So, we will input the structure in parenthesis as we will include three elements in it because, at present, our data structure has two dimensions, i.e., X_train comprise of two dimensions, which is the NumPy array of 1198 lines and 60 columns. Therefore now we will add a new dimension due to which there will be like 3D shape encompassing a new dimension that corresponds to the indicator, and we have to visualize it.\n",
    "The first dimension that we will add is the X_train.shape[0] as it will help us to get the exact number of lines of X_train and then to get the number of timesteps, which is exactly the number of columns, we will get it with the help of X_train.shape[1] as it gives the number of columns that further corresponds to the number of timesteps. And the last dimension will be 1 as we have only a single indicator, but they can be changed in case there are several indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13ec42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping  \n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0cd43",
   "metadata": {},
   "source": [
    "Now that we are done with the data preprocessing, we will now move on to part2, i.e., building the Recurrent Neural Network, where we will build the whole architecture of our stacked LSTM with several LSTM layers.\n",
    "\n",
    "### Part 2 - Building the RNN\n",
    "\n",
    "In the second part, we are going to build the whole architecture of the neural network, a robust architecture, because we are not only going to make a simple LSTM but a stacked LSTM with some dropout regularization to prevent overfitting.\n",
    "\n",
    "So, we will now import the Sequential class that will help us in creating the neural network object representing a sequence of layers, but also the Dense class to add the output layer. We will also import the LSTM class to add the LSTM layers and then the Dropout class to add some dropout regularization. This is all that we need to build a powerful RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3dbd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.layers import LSTM  \n",
    "from keras.layers import Dropout  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21855ed6",
   "metadata": {},
   "source": [
    "Using the TensorFlow backend, all the classes are imported, as shown above.\n",
    "\n",
    "Next, we will initialize our Recurrent Neural Network as a sequence of layers as opposed to a computational graph. We will use the Sequential class from the Keras to introduce the regressor as a sequence of layers. Regressor is nothing but an object of the sequential class that represents the exact sequence of the layers.\n",
    "\n",
    "We are calling it as regressor as opposed to the classifiers in ANN and CNN models because this time, we are predicting a continuous output, or we can say a continuous value, which is the Google stock price at time t+1. So, we can say that we are doing a regression, which is all about predicting continuous value, whereas classification was predicting a category or a class, and since we are predicting a continuous value, this is the reason why we called our Recurrent Neural Network a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "824883ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN  \n",
    "regressor = Sequential()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f5995",
   "metadata": {},
   "source": [
    "After initializing the regressor, we will add different layers to make it a powerful stacked LSTM. So, we will start by adding the first LSTM layer of our Recurrent Neural Network, which was introduced as a sequence of layers and also some dropout regularization so as to avoid overfitting as we don't want while predicting the stock price. We will do this in two steps: we will add the first LSTM layer, and then we will add the dropout regularization.\n",
    "\n",
    "Let's starts with adding our first LSTM layer, and for that, we will take our regressor, which is an object of the sequential class. The sequential class contains the add method that allows adding some layers of the neural network, and inside the add method, we will input the type of layer that we want to add, i.e., an LSTM layer and that is where we use the LSTM class because actually what we are adding in this add method will be an object of the LSTM class. Therefore, we create the LSTM layer by creating an object of the LSTM class, which will take several arguments that are as follows:\n",
    "\n",
    "* The first argument is the number of units, which is the number of LSTM cells or memory unit, but for simplicity, we call it neurons that we want to have in this LSTM layer. So, we will choose a relevant number. Even if we want to stack so many layers, we want our model to have high dimensionality. So, indeed we are making the high dimensionality with the help of LSTM layers that we are going to add, but we can even increase its dimensionality by including a large number of neurons in each of the LSTM layers. Since capturing the trends of the stock price is pretty much complex and we need to have this high dimensionality for which we also need to have a large number of neurons in each of the multiples of LSTM layers. Therefore, we will choose 50 neurons for this LSTM layers because if we have chosen too little neurons, then they would not have captured the upward and forward trends, but as we already selected 50 neurons, it will definitely lead to a better result.\n",
    "* Then the second argument is the return_sequences, which we have to set it equal to True because we are building a stacked LSTM that further contains several LSTM layers and when we add another LSTM layer after creating the first one, then we will need to set the return_sequences argument equals to True. Once we are done with adding the LSTM layers, such that we will not incorporate more layers, then we will set it to False. But we would not do it because it's a default value of the return_sequences parameter.\n",
    "* Lastly, the third argument is the input_shape, which is exactly the shape of the input containing the X_train that we created in the last step of the data preprocessing part. It is an input shape in the 3-dimension corresponding to the observation, the timesteps, and the indicators. But in the third argument of the LSTM class, we are not required to add the three dimensions, only the two last ones corresponding to the timesteps and indicators because the first one corresponds to the observations that will be automatically taken into account. So, we will only specify the X_train.shape[1] that corresponds to the timesteps, and [1] corresponds to the predictors or indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a899c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first LSTM layer and some Dropout regularization  \n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b50fd4",
   "metadata": {},
   "source": [
    "After we are done with our first step, now we will take care of the second sub-step of the first step building the architecture of the neural network, i.e., adding some Dropout regularization and to do this, we will again take our regressor followed by using the add method of the sequential class because it will work the same way as for the LSTM. We will start by creating an object of the Dropout class that we already imported to include this dropout regularization.\n",
    "\n",
    "Therefore, exactly as for LSTM, we need to specify here the name of this class as Dropout that will take only one argument, i.e., Dropout rate, which is nothing but the number of neurons that we want to drop or in simple words that we want to ignore in the layer to do this regularization. And the relevant number to use them is to drop 20% of the neurons in the layer, which is exactly that we need to input here. This is the reason why we have added 0.2 as it corresponds to 20%.\n",
    "\n",
    "So, we will 20% of dropout, i.e., 20% of the neurons of the LSTM layer will be ignored during the training that is during forward and backward propagation happening in each iteration of training. Therefore, since 20% of 50 is 10 neurons, which simply means that 10 neurons will be ignored and dropped out during each iteration of the training. Hence, we are done here with our first LSTM layer, to which we added some dropout regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cf3530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(Dropout(0.2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6ce9a",
   "metadata": {},
   "source": [
    "Now we will add some extra LSTM layers followed by adding to each of them some dropout regularization. So, we will start by adding our second LSTM layer in the same way as we did in the previous step because we will again use the add method from the sequential class to add a new LSTM layer and some dropout regularization to our regressor, but we will do some changes to the input_shape argument. As in the previous step had to specify the input_shape argument because that was our first LSTM layer and we were required to specify the shape of the input with the last two dimensions corresponding to the timesteps and the predictors, but now things are slightly different, we are just adding our second LSTM layer, which is why we don't need to specify it anymore. Since it is automatically recognized, so will skip adding it to the code when we are adding our next LSTM layers after the first one.\n",
    "\n",
    "Therefore, we will keep the same number of neurons in the second LSTM layer, i.e., 50 neurons, as well as the same 20% dropout for the regularization due to the fact that it's a relevant choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1544de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularization  \n",
    "regressor.add(LSTM(units = 50, return_sequences = True))  \n",
    "regressor.add(Dropout(0.2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44d4ec",
   "metadata": {},
   "source": [
    "Similarly, in order to add our third LSTM layer, we will exactly copy the above two lines of code that added out the second LSTM layer because adding the third LSTM layer is similar to that of adding the second LSTM layer. We simply need to specify the number of neurons in the LSTM layer, which we are keeping it as 50 neurons so as to have the same goal of having a high dimensionality. We still need to keep return_sequences equal to True because we are adding another LSTM layer after the second LSTM layer, and again, we will keep 20% dropout regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dadc3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularization  \n",
    "regressor.add(LSTM(units = 50, return_sequences = True))  \n",
    "regressor.add(Dropout(0.2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c6b70",
   "metadata": {},
   "source": [
    "Next, we will add our fourth LSTM layer, but this time things will be slightly changed. We will keep 50 neurons in this fourth LSTM layer because this is not the final layer of the Recurrent Neural layer. But after the fourth layer, we will have our output layer with the output dimension, which will be 1, of course, as we are predicting only one value, the value of the stock price at time t+1. Since we are adding the fourth LSTM layer, which is the last LSTM layer that we are adding, so we will need to set the return_sequences equal to False because we are not going to return any more sequences. But as we know, the default value for the return_sequences parameter is False, so we will just remove that part as that is what we have to do for the fourth LSTM layer.\n",
    "\n",
    "We are just adding the LSTM class with 50 units, and the same we will keep the 20% dropout regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4b4751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularization  \n",
    "regressor.add(LSTM(units = 50))  \n",
    "regressor.add(Dropout(0.2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582783ff",
   "metadata": {},
   "source": [
    "Now we just need to add our final layer, which is the output layer. We will simply take our regressor, which is exactly the same as for the ANN and CNN, followed by adding the add method again from sequential class to add the final output layer of our neural network. Since we are not adding the LSTM layer, but actually a classic fully connected layer because the output layer is fully connected to the previous LSTM layer, so in that case to make it a fully connected layer, we will need to use the Dense class exactly as we did for the ANN and CNN.\n",
    "\n",
    "So, we will specify the Dense class in the add method, and then we will add one argument which will correspond to the number of neurons that are needed to be in the output layer. Since we are predicting a real value corresponding to the stock price, so the output has only one dimension, which is exactly what we need to input and the argument for that is units as it corresponds to the number of neurons in the output layer or the dimension of the output layer, which is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cea1a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer  \n",
    "regressor.add(Dense(units = 1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7cbf7",
   "metadata": {},
   "source": [
    "Now that we are done with the architecture of our super robust LSTM recurrent neural network, we have two remaining steps of Building the RNN; first one is compiling the RNN with a powerful optimizer and the right loss, which will be the mean squared error because we are doing some regression and the second step is to fit this recurrent neural network to the training set.\n",
    "\n",
    "Since our training set is composed of X_train, which is the right data structure expected by the neural networks, so we will take X_train instead of the training set or the training_set_scaled, and of course, we will need to specify the outputs when fitting the regressor to our training sets because the output contains the ground truth that is the stock price at time t+1. As we are training the RNN on the truth, i.e., the true stock price that is happening at time t+1 after the 60 produced stock prices during the 60 produced financial days, so that's why we also need to include the ground truth, i.e., y_train.\n",
    "\n",
    "Let's compile the RNN with the right optimizer and the right loss function. So, we will start by taking our regressor as we are predicting a continuous value followed by using the compile method, which is another method of a sequential class, and inside the compile method, we will input two arguments, i.e., the optimizer and the loss function.\n",
    "\n",
    "In general, for recurrent neural network, an RMS prop optimizer is recommended, but in our case of a problem, we will be using adam optimizer because it's always a safe choice as it very powerful and always perform some relevant updates of the weights. And the second argument that we will input is the loss function. Since we are not dealing with the classification problem, but the regression problem because we have to predict a continuous value, so this time the loss function is mean_squared_error due to the fact that the error can be measured by the mean of the squared differences between the predictions and targets, i.e., the real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7112cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN  \n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232bc89",
   "metadata": {},
   "source": [
    "After compiling the RNN, we will fit the RNN to the training set that is composed of X_train and y_train. So, we will again start by taking the regressor and not the classifier followed by using the fit method, which will not only connect the neural network to the training set but will also execute the training over a certain number of epochs that we will choose in the same fit method. Inside the fit method, we will pass four arguments that are the X_train, y_train, epochs, and the batch_size. So, our network is going to be trained not on the single observation going to the neural network but on the batches of observation, i.e., the batches of the stock prices going into the neural network.\n",
    "\n",
    "Instead of updating the weights every stock price being forward propagated into the neural network and then generating an error, which is backpropagated into the neural network, we will do that for every 32 stock prices because we have chosen the batch_size of 32. So, here we are done with building a super robust recurrent neural network as well as we are ready to train it on 5 years of the Google Stock Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97ca2f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 8s 66ms/step - loss: 0.0480\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 3s 90ms/step - loss: 0.0063\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 4s 92ms/step - loss: 0.0054\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0051\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0050\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0051\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0042\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 3s 76ms/step - loss: 0.0047\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0042\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0043\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0042\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0039\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0044\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0039\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0039\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0039\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0033\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0040\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 3s 87ms/step - loss: 0.0037\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 3s 92ms/step - loss: 0.0040\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 3s 88ms/step - loss: 0.0034\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 3s 77ms/step - loss: 0.0033\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0033\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0036\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 0.0032\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0031\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0030\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0033\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0033\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 0.0036\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0029\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0032\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0029\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0029\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0031\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 0.0029\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0026\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 0.0036\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0031\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0027\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0029\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0027\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0027\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0024\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0027\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0025\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 0.0029\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0023\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0025\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 0.0023\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0023\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0024\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0023\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0023\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0024\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0022\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0022\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0021\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 3s 78ms/step - loss: 0.0022\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 3s 79ms/step - loss: 0.0022\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0019\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0020\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0022\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0018\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0020\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0020\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0019\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 0.0022\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0019\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0017\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0020\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 3s 69ms/step - loss: 0.0019\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0018\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0016\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0019\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0021\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 3s 73ms/step - loss: 0.0018\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0018\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0017\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0016\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0016\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0018\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0016\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0017\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0016\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0018\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0017\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 3s 74ms/step - loss: 0.0015\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 3s 72ms/step - loss: 0.0019\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0018\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 3s 75ms/step - loss: 0.0016\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 3s 80ms/step - loss: 0.0018\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0015\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0016\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 3s 70ms/step - loss: 0.0014\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0015\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 3s 71ms/step - loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f48157a020>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set  \n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ca7a7",
   "metadata": {},
   "source": [
    "From the above image, we can see that we have prevented enough the overfitting to not decrease the loss even more because if we had obtained too small loss in the end, we might have got some overfitting, as well as our predictions, will be closed to the real Google stock price. In the training data, which is the data of the past but not the one in which we are interested in making predictions, we will get some great loss on it and some really bad loss on the test data. So, this is exactly what overfitting is all about.\n",
    "\n",
    "This is the only reason when we train the training set, we must be careful not to obtain overfitting and therefore not to try to decrease the loss as much as possible, which is why it seems that we get really good results.\n",
    "\n",
    "After this, we will move on to the 3rd part in which we will visualize our predictions compared to the real Google stock price of the first financial month of 2017.\n",
    "\n",
    "### Part 3 - Making the predictions and visualizing the results\n",
    "\n",
    "First, we will get the real stock price of 2017, then in the second step, we will get the predicted stock price of 2017, and lastly, we will visualize the results. So, in order to get the real stock price of 2017, we will get it from the test set in the CSV file, and therefore we will just do exactly the same as what we did for our training set.\n",
    "\n",
    "We will simply start with creating a data frame by importing the Google_Stock_Price_Test.csv file with the read_csv function by pandas, and then we will select the right column, the open google stock price followed by making it a NumPy array that we will do by replacing the training set by the test set. Since the test set is going to be the real values of the Google stock price in the first month of\n",
    "January 2017, so we will simply replace the training_set by the real_stock_price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2706bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the real stock price of 2017  \n",
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')  \n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee535bdc",
   "metadata": {},
   "source": [
    "Next, we will move onto our second step in which we will the predicted stock price of January 2017. So, here we will use our regressor with the help of which we are going to predict the Google stock prices of January 2017. Basically, the first key point is that we trained our model to predict the stock price at time t+1 based on the 60 previous stock prices and therefore to predict each stock price of each financial day of January 2017, we will need the 60 previous stock prices of 60 previous financial days before the actual day.\n",
    "\n",
    "Then the second key point is, in order to get at each day of January 2017, the 60 previous stock prices of the 60 previous days, we will need both the training set as well as the test set because we will have some of the 60 days that will be from the training set as they we will be from December 2016, and we will also have some stock prices of the test set due to the fact that some of them will come from January 2017.\n",
    "\n",
    "Therefore, the first thing that we need to do is some concatenation of the training set and the test set to be able to get these 60 previous inputs for each day of January 2017, which then leads to understanding the third key point. We will be making this concatenation by concatenating the training set and the test set, i.e., by concatenating the training set that contains the real Google stock prices from 2012 to the end of 2016, such that is concatenated this training set with the test set will actually lead to a problem because then we will have to scale this concatenation of the training set and the test set. To do that, we will have to apply the fit_transform method from the sc object that we created in the feature scaling section to scale the concatenation of the training set and the test set to get the scaled real_stock_price. But it will change the actual test values, and we should never do this, so we will keep the actual test values as they are.\n",
    "\n",
    "Therefore, we will make another concatenation, which will be to concatenate the original DataFrames that we still have, i.e., dataset_train and dataset_test and from this concatenation, we will get the inputs of each prediction, which is the produced stock prices at each time t and this is what we will scale. These are those inputs that we will apply on our sc object as well as scale to get the predictions. In this way, we are only scaling the input instead of changing the actual test values and will lead us to the most relevant results.\n",
    "\n",
    "So, we will start by introducing a new variable called dataset_total as it will contain the whole dataset, and then we will do concatenation for which we will use the concat function from the pandas library. Inside the pandas function, we need to input two arguments such as the first one is the pair of two DataFrames that we want to concatenate, i.e., we will concatenate the dataset_train to the dataset_test, and the other argument is the axis along which we want to make this concatenation. Since we want to make this concatenation along lines as we want to add the stock prices of the test set to that of the training set, so we will make the concatenation along the vertical axis and to specify this we will add the second argument, i.e., axis=0 because the vertical axis is labeled by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "019b3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2017  \n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942170fc",
   "metadata": {},
   "source": [
    "Now in the next step, we will get the inputs, i.e., at each time t or each financial day of January 2017, we need to get the 60 previous stock prices of the 60 previous financial days. So, to get these inputs, we will start by introducing new variable inputs. Then we will get the dataset_total because we are getting these stock prices from our DataFrame, dataset by so far, and therefore as we will need the stock prices from the first financial day of 2017 minus 60, up to the last stock price of our whole dataset.\n",
    "\n",
    "For that, we get our first lower bound of this range of inputs that we need. The lower bound is the stock price at January 3rd minus 60 and to get that we will need to find the index of January 3rd, which will simply do by taking len(dataset_total), which is the length of the total dataset followed by subtracting it to the len(dataset_test), which is the length of the dataset and as we want to get the stock price at this day, so we will see again minus it by 60 because it is the lower bound of the inputs that we require. And to get the upper bound, we will simply need to add a colon (i.e. :). Basically, the upper bound is the last index of the whole dataset because to predict the last stock price of the last financial day, we will need the 60 previous stock prices, and therefore the last stock prices we will need is the stock price just before that last financial day. So, this is the range of inputs that will result in the DataFrame, but of course, we need to move on to NumPy arrays, and for that, we will add dot values to make it a NumPy array. All of these will contain all the inputs that we will need to predict the stock prices of January 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3105b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6dcdc",
   "metadata": {},
   "source": [
    "In the next step, we will make the simple reshape to get the right NumPy shape, so we will update the inputs, and to do that, we will again take the same old inputs that we took in the previous step to which we will further add the reshape function. Inside the reshape function, we will pass (-1, 1) as it will help us to get inputs with different stock prices of January 3rd - 3 months up to the final stock prices in lines and in 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c942748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ae36d",
   "metadata": {},
   "source": [
    "Now we will repeat the same process that we did before also to obtain the right 3D format, which is expected by the neural network not only for training but for the predictions too. So, whether we apply the fit method to train the regressor or to predict method to make the regressor predict something and for that, we need to have the right format of inputs, which is the 3D format that we made previously. Before starting with making this 3D special structure, we have to scale our inputs because they are directly coming from the original DataFrames contained in dataset_total, so we have the original values of the stock prices and since our recurrent neural network was trained on the scaled values, well, of course, we need to scale the inputs, which satisfies here the 3rd key point that we discussed earlier, i.e., to scale the inputs only and not the actual test values because we need to keep the test values as it is.\n",
    "\n",
    "So, we will start by updating the inputs again for which we will the scaling object, which is sc, but here we will not use the fit_transform method because the sc object is already fitted to the training set due to which we will directly use the transform method as the scaling we need to apply to our input must be the same scaling that we applied to the training set. Therefore, we must not fit our scaling object sc again, but we must directly apply the transform method to get the previous scaling on which our regressor was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a42b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sc.transform(inputs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6fe4e4",
   "metadata": {},
   "source": [
    "Next, we will create a special data set structure for the test set, so we will introduce a new variable and call it as X_test because it will be the input that we will need to predict the value of the test set. Since we are not doing any training, so we would need the y_test. We are actually doing some predictions, so we don't need a ground truth anymore, which is why y_train is also not included here and inside the loop, we will not change the lower bound to get 60 previous time steps, and since we are i-60 here, so we must start at 60. But then for the upper bound, things are quite different because all that we are doing is to get the input for the test set as it contains only 20 financial days, so we need to go up to 60+20=80, and with this, we will get our 60 previous inputs for each of the stock prices of January 2017 that contains 20 financial days.\n",
    "\n",
    "After this we will append in X_test, the previous stock prices, which are indeed taken from the inputs and keep its range of the indexes from i-60 to i, we are also keeping 0 as it corresponds to the Open Google Stock Prices and anyway there is only one column in the inputs.\n",
    "\n",
    "Since X_test is also a list, so we again need to make it a NumPy array so that it can be accepted by our future Recurrent Neural Network and by doing this we have a structure where we have in each line of observations, i.e., for each stock prices of January 2017, we have in 60 columns the 60 previous stock prices that we need to predict the next stock price.\n",
    "\n",
    "Now we will further move on to get the 3D format for which we will again use the reshape function to add a dimension in NumPy array. We will do in the exact same way as we did in the reshaping section of Data preprocessing part, just we need to replace X_train by X_test and the rest code as well as its explanation is similar as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "623c4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []  \n",
    "for i in range(60, 80):  \n",
    "    X_test.append(inputs[i-60:i, 0])  \n",
    "X_test = np.array(X_test)  \n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b336ba",
   "metadata": {},
   "source": [
    "So, we are ready to make predictions as we have right 3D structure of our inputs contained in X_test, which is exactly what is expecting our recurrent neural network regressor and therefore we are ready to apply our predict method from this regressor to get our predicted stock prices of January 2017.\n",
    "\n",
    "We are going to take the regressor, and from this regressor, we will apply the predict method to which we need to input the X_test that contains the inputs in the right format to predict the stock prices of January 2017. Since it returns predictions, so we will store these predictions in a new variable named as predicted_stock_price that will be consistent with the real_stock_price followed by making it equal to what is returned by the predict method taken from our regressor and apply it to the right input contained in the X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db832523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec5dc1",
   "metadata": {},
   "source": [
    "After doing this, we will inverse the scaling of our predictions because our regressor was trained to predict the scaled values of the stock price, so in order to get the original scale of these scaled predicted values, we simply need to apply the inverse_transform method from our scaling sc object. Since we are going to update the predicted_stock_price with the right scale of our Google stock price values, so we will get our predicted_stock_price followed by taking our scaling object, i.e., sc and that's where we will apply the inverse_transform method to which we are going to apply the predicted_stock_price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2749a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc79056",
   "metadata": {},
   "source": [
    "So, after executing the whole \"Getting the predicted stock price of 2017\" section, we will get the above output that contains the predictions, which is indeed in the range of Google Stock Prices in the month of January 2017. But we cannot realize it yet if it followed approximately the trend of the real Google Stock Price in January 2017.\n",
    "\n",
    "Next, we will move on to visualizing the results, which will actually witness the robustness of the model as we are going to see how our predictions follow the trends of the Google Stock Prices. Therefore, we will start by using the plt.plot function from the matplotlib.pyplot library and inside this plt.plot function, we will first need to input the name of the variable that contains the stored stock prices, which we want to plot, and these are contained in the real_stock_price variable. So, we first need to input the real_stock_price variable followed by adding our next argument, i.e., the color which we have chosen red for the real stock price, and then the last argument is the label for which we will plot some legends on the chart. Therefore we will use plt.legend function to display the legends. Here we have chosen 'Real Google Stock Price', so as to keep in mind that we are plotting not the whole real Google stock price between 2012 and the first month of 2017 instead we are plotting the real Google stock price in the first month of January 2017 because we only have the predictions of January 2017, so we just want to compare these two stock prices during this first month.\n",
    "\n",
    "Similarly, we will again use the plt.plot function to plot the predicted_stock_price variable that contains the stored predictions of the stock price for January 2017. It will be carried out in the same way as we did above, but will choose a different color, i.e., blue and label that is 'Predicted Google Stock Price'.\n",
    "\n",
    "Since we want to have a nice chart, so we will add a title to the chart for which we will use the plt.title function, and inside it, we will mention the title that we want to give to our chart, i.e. 'Google Stock Price Prediction'.\n",
    "\n",
    "Next, we will add the label to the x-axis as well as the y-axis, and to do that, we will use the plt.xlabel and plt.ylabel functions, each, respectively. Inside the plt.xlabel function, we will input the label that corresponds to the x-axis, i.e., 'Time' as we are plotting from 3rd January to 31st January and similarly inside the plt.ylabel, we will input the label that corresponds to the y-axis, i.e. 'Google Stock Price'.\n",
    "\n",
    "After this, we will add plt.legend function without any input so that we can include the legends in the chart followed by ending up with plt.show function to display the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba82fcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the results  \n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')  \n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')  \n",
    "plt.title('Google Stock Price Prediction')  \n",
    "plt.xlabel('Time')  \n",
    "plt.ylabel('Google Stock Price')  \n",
    "plt.legend()  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceec1b5",
   "metadata": {},
   "source": [
    "From the above input, we can see that we have the real Google stock price in red and our predicted Google stock price in blue. We also get a comparison of the real and the predicted Google stock prices for the whole month of January 2017. We have got the real Google stock price from the verified financial sources from the web. However, the predictions are coming from the RNN model that we just implemented.\n",
    "\n",
    "We can see in some parts our predictions are lagging behind the actual values. We can clearly see a big spike, like a stock time singularity, which is not followed by the predictions, and it is completely normal. Out model just lags behind because it cannot react to fast, nonlinear changes.\n",
    "\n",
    "The spike in the image is the stock time irregularity, is indeed a fast, nonlinear change to which our model cannot react properly, but that's totally fine because according to the Brownian Motion Mathematical Concept in financial engineering, the future variations of the stock price are independent of the past. And, therefore, the future variation that we see around the spike, well it is a variation that is indeed totally independent from the previous stock prices.\n",
    "\n",
    "But on the other hand, there is good news that out model reacts okay to smooth changes that happen on the Real Google Stock Price except for the spikes to which our model cannot react, but other than that, our Recurrent Neural Network reacts pretty well to these smooth changes.\n",
    "\n",
    "So, it can be concluded that in the parts of the predictions containing some spikes, well our predictions lag behind the actual values because our model cannot react to fast, nonlinear changes, whereas on the other hand, for the parts of the predictions containing smooth changes, well our model predicts pretty well as well as manages to follow the upward and downward trends. It manages to follow the upward trend, the stable trend, and again the upward trend on the Predicted Google Stock Price. Then there is a downward trend in the last financial days of January, and it started to capture it. So, we can say it make really good results that actually make pretty much sense in spite of spikes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
