{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6d2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ac6b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5ea3388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, 3:13].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b0f1e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.iloc[:, 13].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7d1811c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[228, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [217, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [111, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [318, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [381, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [401, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  \n",
    "labelencoder_X = LabelEncoder()  \n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68b40859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[228, 0, 'Female', ..., 1, 1, 101348.88],\n",
       "       [217, 2, 'Female', ..., 0, 1, 112542.58],\n",
       "       [111, 0, 'Female', ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [318, 0, 'Female', ..., 0, 1, 42085.58],\n",
       "       [381, 1, 'Male', ..., 1, 0, 92888.52],\n",
       "       [401, 0, 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  \n",
    "labelencoder_X_1 = LabelEncoder()  \n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "243f4be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[228, 0, 0, ..., 1, 1, 101348.88],\n",
       "       [217, 2, 0, ..., 0, 1, 112542.58],\n",
       "       [111, 0, 0, ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [318, 0, 0, ..., 0, 1, 42085.58],\n",
       "       [381, 1, 1, ..., 1, 0, 92888.52],\n",
       "       [401, 0, 0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder_X_2 = LabelEncoder()  \n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde663a",
   "metadata": {},
   "source": [
    "Since there is no relational order between the categories of our categorical variable, so for that we need to create a dummy variable for the country categorical variable as it contains three categories unlike the gender variable having only two categories, which is why we will be removing one column to avoid the dummy variable trap. It is useless to create the dummy variable for the gender variable. We will use the OneHotEncoder class to create the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f5b48de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[228, 0, 0, ..., 1, 1, 101348.88],\n",
       "       [217, 2, 0, ..., 0, 1, 112542.58],\n",
       "       [111, 0, 0, ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [318, 0, 0, ..., 0, 1, 42085.58],\n",
       "       [381, 1, 1, ..., 1, 0, 92888.52],\n",
       "       [401, 0, 0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer  \n",
    "label_encoder_x_1 = LabelEncoder()  \n",
    "X[: , 2] = label_encoder_x_1.fit_transform(X[:,2])  \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2deded25",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    transformers=[  \n",
    "        (\"OneHot\",        # Just a name  \n",
    "         OneHotEncoder(), # The transformer class  \n",
    "         [1]              # The column(s) to be applied on.  \n",
    "         )  \n",
    "    ],  \n",
    "    remainder='passthrough' # don't apply anything to the remaining columns   \n",
    ")  \n",
    "X = transformer.fit_transform(X.tolist())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8b8fa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 12)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e7e84d",
   "metadata": {},
   "source": [
    "By having a look at X, we can see that all the columns are of the same type now. Also, the type is no longer an object but float64. We can see that we have twelve independent variables because we have three new dummy variables.\n",
    "\n",
    "Next, we will remove one dummy variable to avoid falling into a dummy variable trap. We will take a matrix of features X and update it by taking all the lines of this matrix and all the columns except the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cee021b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 11)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[:, 1:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8c585",
   "metadata": {},
   "source": [
    "It can be seen that we are left with only two dummy variables, so no more dummy variable trap.\n",
    "\n",
    "Now we are ready to split the dataset into the training set and test set. We have taken the test size to 0.2 for training the ANN on 8,000 observations and testing its performance on 2,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "566312b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00871bcf",
   "metadata": {},
   "source": [
    "Besides parallel computations, we are going to have highly computed intensive calculations as well as we don't want one independent variable dominating the other one, so we will be applying feature scaling to ease out all the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93b68cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953bcf99",
   "metadata": {},
   "source": [
    "### Building an ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b5d1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7348ea",
   "metadata": {},
   "source": [
    "After importing the Keras library, we will now import two modules, i.e., the Sequential module, which is required to initialize our neural network, and the Dense module that is needed to build the layer of our ANN.\n",
    "\n",
    "Next, we will initialize the ANN, or simply we can say we will be defining it as a sequence of layers. The deep learning model can be initialized in two ways, either by defining the sequence of layers or defining a graph. Since we are going to make our ANN with successive layers, so we will initialize our deep learning model by defining it as a sequence of layers.\n",
    "\n",
    "It can be done by creating an object of the sequential class, which is taken from the sequential model. The object that we are going to create is nothing but the model itself, i.e., a neural network that will have a row of classifiers because we are solving a classification problem where we have to predict a class, so our neural network model is going to be a classifier. As in the next step, we will be predicting the test set result using the classifier name, so we will call our model as a classifier that is nothing but our future Artificial Neural Network that we are going to build.\n",
    "\n",
    "Since this classifier is an object of Sequential class, so we will be using it, but will not pass any argument because we will be defining the layers step by step by starting with the input layer followed by adding some hidden layers and then the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4ebe5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f9af9",
   "metadata": {},
   "source": [
    "After this, we will start by adding the input layer and the first hidden layer. We will take the classifier that we initialized in the previous step by creating an object of the sequential class, and we will use the add() method to add different layers in our neural network. In the add(), we will pass the layer argument, and since we are going to add two layers, i.e., the input and first hidden layer, which we will be doing with the help of Dense() function that we have mentioned above.\n",
    "\n",
    "Within the Dense() function we will pass the following arguments;\n",
    "\n",
    "* units are the very first argument, which can be defined as the number of nodes that we want to add in the hidden layer.\n",
    "* The second argument is the kernel_initializer that randomly initializes the weight as a small number close to zero so that they can be randomly initialized with a uniform function. Here we have a simple uniform function that will initialize the weight according to the uniform distribution.\n",
    "* The third argument is the activation, which can be understood as the function that we want to choose in our hidden layer. So, we will be using the rectifier function for the hidden layers and the sigmoid function for the output layer. Since we are in the hidden layer, we are using the \"relu\" perimeter as it corresponds to the rectifier function.\n",
    "* And the last is the input_dim argument that specifies the number of nodes in the input layer, which is actually the number of independent variables. It is very necessary to add the argument because, by so far, we have only initialized our ANN, we haven't created any layer yet, and that's why it doesn't know which node this hidden layer we are creating is expecting as inputs. After the first hidden layer gets created, we don't need to specify this argument for the next hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2336cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e6329d",
   "metadata": {},
   "source": [
    "Next, we will add the second hidden layer by using the same add method followed by passing the same parameter, which is the Dense() as well as the same parameters inside it as we did in the earlier step except for the input_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27963827",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdc447",
   "metadata": {},
   "source": [
    "After adding the two hidden layers, we will now add the final output layer. This is again similar to the previous step, just the fact that we will be units parameter because in the output layer we only require one node as our dependent variable is a categorical variable encompassing a binary outcome and also when we have binary outcome then, in that case, we have only one node in the output layer. So, therefore, we will put units equals to 1, and since we are in the output layer, we will be replacing the rectifier function to sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fca6d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f18b97",
   "metadata": {},
   "source": [
    "As we are done with adding the layers of our ANN, we will now compile the whole artificial neural network by applying the stochastic gradient descent. We will start with our classifier object, followed by using the compile method and will pass on the following arguments in it.\n",
    "\n",
    "* The first argument is the optimizer, which is simply the algorithm that we want to use to find the optimal set of weights in the neural networks. The algorithm that we are going to use is nothing but the stochastic gradient descent algorithm. Since there are several types of stochastic descent algorithms and the most efficient one is called \"adam,\" which is going to be the input of this optimizer parameter.\n",
    "* The second parameter is the loss, which is a loss function within the stochastic gradient descent algorithm, which is used to find the optimal weights. Since our dependent variable has a binary outcome, so we will be using binary_crossentropy logarithmic function, and when there is a binary outcome, then we will incorporate categorical_crossentropy.\n",
    "* The last argument will be the metrics, which is nothing but a criterion to evaluate our model, and we are using the \"accuracy.\" So, what happens is when the weights are updated after each observation, the algorithm makes use of this accuracy to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "338e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888df76",
   "metadata": {},
   "source": [
    "Next, we will fit the ANN to the training set for which we will be using the fit method to fit our ANN to the training set. In the fit method, we will be passing the following arguments:\n",
    "\n",
    "* The first argument is the dataset on which we want to train our classifier, which is the training set separated into two-argument such as X_train (matrix of feature containing the observations of the train set) and y_train (containing the actual outcomes of the dependent variable for all the observations in the training set).\n",
    "* The next argument is the batch_size, which is the number of observations, after which we want to update the weight.\n",
    "* And lastly, the no. of epochs that we are going to apply to see the algorithm in action as well the improvement in accuracy over the different epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3feb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 2s 1ms/step - loss: 0.4768 - accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 1s 944us/step - loss: 0.4266 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8104\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4180 - accuracy: 0.8238\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4159 - accuracy: 0.8261\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 1s 906us/step - loss: 0.4140 - accuracy: 0.8300\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4130 - accuracy: 0.8306\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4116 - accuracy: 0.8335\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4100 - accuracy: 0.8326\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4101 - accuracy: 0.8329\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4096 - accuracy: 0.8332\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4086 - accuracy: 0.8331\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4084 - accuracy: 0.8330\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4084 - accuracy: 0.8331\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4070 - accuracy: 0.8357\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4072 - accuracy: 0.8324\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4069 - accuracy: 0.8326\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4068 - accuracy: 0.8341\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4060 - accuracy: 0.8355\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4058 - accuracy: 0.8340\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4058 - accuracy: 0.8353\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4054 - accuracy: 0.8332\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4053 - accuracy: 0.8324\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4053 - accuracy: 0.8314\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.4051 - accuracy: 0.8338\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4050 - accuracy: 0.8341\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4049 - accuracy: 0.8334\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4046 - accuracy: 0.8349\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4044 - accuracy: 0.8342\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4040 - accuracy: 0.8332\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4044 - accuracy: 0.8334\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4041 - accuracy: 0.8341\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4033 - accuracy: 0.8359\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4037 - accuracy: 0.8351\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4037 - accuracy: 0.8357\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4034 - accuracy: 0.8347\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4041 - accuracy: 0.8349\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4039 - accuracy: 0.8340\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4038 - accuracy: 0.8334\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4037 - accuracy: 0.8338\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4034 - accuracy: 0.8336\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4032 - accuracy: 0.8346\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4033 - accuracy: 0.8359\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4032 - accuracy: 0.8341\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4029 - accuracy: 0.8334\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4035 - accuracy: 0.8346\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4029 - accuracy: 0.8341\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4029 - accuracy: 0.8344\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4031 - accuracy: 0.8338\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4030 - accuracy: 0.8351\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4032 - accuracy: 0.8324\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8342\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4030 - accuracy: 0.8322\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8330\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4029 - accuracy: 0.8340\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4029 - accuracy: 0.8366\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8344\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8339\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8341\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8335\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4029 - accuracy: 0.8321\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.8334\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4028 - accuracy: 0.8350\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4029 - accuracy: 0.8332\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4028 - accuracy: 0.8341\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4026 - accuracy: 0.8324\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8349\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4031 - accuracy: 0.8336\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8335\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8346\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8313\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4030 - accuracy: 0.8345\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4021 - accuracy: 0.8329\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4018 - accuracy: 0.8336\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8340\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8339\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8332\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4018 - accuracy: 0.8338\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8341\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4021 - accuracy: 0.8336\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 960us/step - loss: 0.4013 - accuracy: 0.8351\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 1s 882us/step - loss: 0.4021 - accuracy: 0.8335\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8353\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.8336\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8342\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8324\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4019 - accuracy: 0.8322\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8339\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4021 - accuracy: 0.8338\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4022 - accuracy: 0.8338\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8331\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4021 - accuracy: 0.8357\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4019 - accuracy: 0.8334\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4014 - accuracy: 0.8336\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4018 - accuracy: 0.8339\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4014 - accuracy: 0.8341\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4013 - accuracy: 0.8339\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4015 - accuracy: 0.8334\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 1s 948us/step - loss: 0.4015 - accuracy: 0.8361\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 1s 959us/step - loss: 0.4009 - accuracy: 0.8361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d02083f010>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1db8f8",
   "metadata": {},
   "source": [
    "From the output image given above, you can see that our model is ready and has reached an accuracy of 84% approximately\n",
    "\n",
    "### Making the Predictions and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e8e9c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2559072 ],\n",
       "       [0.33862698],\n",
       "       [0.19755672],\n",
       "       ...,\n",
       "       [0.18434273],\n",
       "       [0.13445483],\n",
       "       [0.11231428]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e827bd",
   "metadata": {},
   "source": [
    "From the output image given above, we can see all the probabilities that the 2,000 customers of the test set will leave the bank. For example, if we have a look at first probability, i.e., 25% means that this first customer of the test set, indexed by zero, has a 25% chance to leave the bank.\n",
    "\n",
    "Since the predicted method returns the probability of the customers leave the bank and in order to use this confusion matrix, we don't need these probabilities, but we do need the predicted results in the form of True or False. So, we need to transform these probabilities into the predicted result.\n",
    "\n",
    "We will choose a threshold value to decide when the predicted result is one, and when it is zero. So, we predict 1 over the threshold and 0 below the threshold as well as the natural threshold that we will take is 0.5, i.e., 50%. If the y_pred is larger, then it will return True else False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fcb83cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_pred > 0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d92c913f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1539,   56],\n",
       "       [ 261,  144]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix  \n",
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
